#!/usr/bin/env python3
"""
Debug Report Viewer for CERES RISC-V
=====================================

Interactive viewer and analyzer for debug reports generated by debug_logger.py

Features:
- Pretty-print debug reports
- Search and filter execution steps
- Identify bottlenecks and errors
- Compare multiple runs
- Generate summary statistics

Usage:
    # View latest report
    python3 script/python/debug_viewer.py

    # View specific report
    python3 script/python/debug_viewer.py build/debug_reports/run_20251213_143022_rv32ui-p-add.json

    # Compare two runs
    python3 script/python/debug_viewer.py --compare report1.json report2.json

    # Show only errors
    python3 script/python/debug_viewer.py --errors-only

    # Summary statistics
    python3 script/python/debug_viewer.py --summary
"""

import json
import sys
import argparse
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime


class Colors:
    """ANSI color codes for terminal output."""
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    BLUE = '\033[0;34m'
    CYAN = '\033[0;36m'
    MAGENTA = '\033[0;35m'
    RESET = '\033[0m'
    BOLD = '\033[1m'
    DIM = '\033[2m'


class DebugReportViewer:
    """Viewer for debug reports."""

    def __init__(self, report_path: Path):
        self.report_path = report_path
        with open(report_path) as f:
            self.report = json.load(f)

    def print_header(self):
        """Print report header with metadata."""
        meta = self.report["metadata"]
        env = self.report["environment"]
        result = self.report["result"]

        status_color = Colors.GREEN if result["status"] == "PASSED" else Colors.RED

        print(f"\n{Colors.CYAN}{'='*80}{Colors.RESET}")
        print(f"{Colors.BOLD}DEBUG REPORT: {meta['test_name']}{Colors.RESET}")
        print(f"{Colors.CYAN}{'='*80}{Colors.RESET}")

        print(f"\n{Colors.BOLD}Metadata:{Colors.RESET}")
        print(f"  Test:        {Colors.YELLOW}{meta['test_name']}{Colors.RESET}")
        print(f"  Target:      {meta['target']}")
        print(f"  Makefile:    {meta['makefile']}")
        print(f"  Timestamp:   {meta['timestamp']}")
        print(f"  Session ID:  {meta['session_id']}")
        print(f"  User:        {meta['user']}@{meta['hostname']}")

        if 'git_commit' in meta:
            print(f"  Git:         {meta['git_branch']}@{meta['git_commit']}")

        print(f"\n{Colors.BOLD}Environment:{Colors.RESET}")
        print(f"  CWD:         {env['cwd']}")
        print(f"  Python:      {env['python_version']}")
        if 'verilator_version' in env:
            print(f"  Verilator:   {env['verilator_version']}")

        if env['env_vars']:
            print(f"\n{Colors.BOLD}Environment Variables:{Colors.RESET}")
            for key, val in env['env_vars'].items():
                print(f"    {Colors.DIM}{key}={Colors.RESET}{val}")

        print(f"\n{Colors.BOLD}Result:{Colors.RESET}")
        print(f"  Status:      {status_color}{result['status']}{Colors.RESET}")
        print(f"  Exit Code:   {result['exit_code']}")
        print(f"  Duration:    {result['duration_total_ms']/1000:.2f}s")

        if result['errors']:
            print(f"  {Colors.RED}Errors:      {len(result['errors'])}{Colors.RESET}")
        if result['warnings']:
            print(f"  {Colors.YELLOW}Warnings:    {len(result['warnings'])}{Colors.RESET}")

    def print_execution_flow(self, errors_only: bool = False):
        """Print execution flow with timing."""
        steps = self.report["execution_flow"]

        if not steps:
            return

        print(f"\n{Colors.BOLD}Execution Flow:{Colors.RESET}")
        print(f"{Colors.CYAN}{'─'*80}{Colors.RESET}")

        total_time = sum(s['duration_ms'] for s in steps)

        for step in steps:
            # Skip if errors_only and no errors
            if errors_only and not step['errors']:
                continue

            duration_s = step['duration_ms'] / 1000
            percentage = (step['duration_ms'] / total_time * 100) if total_time > 0 else 0

            # Status color
            if step['exit_code'] == 0:
                status_icon = f"{Colors.GREEN}✓{Colors.RESET}"
            elif step['exit_code'] is None:
                status_icon = f"{Colors.YELLOW}?{Colors.RESET}"
            else:
                status_icon = f"{Colors.RED}✗{Colors.RESET}"

            print(f"\n{status_icon} Step {step['step']}: {Colors.BOLD}{step['name']}{Colors.RESET}")
            print(f"  Type:     {step['type']}")
            print(f"  Duration: {duration_s:.2f}s ({percentage:.1f}%)")

            if step['command']:
                print(f"  Command:  {Colors.DIM}{step['command'][:70]}...{Colors.RESET}")

            if step['args']:
                print(f"  Args:     {Colors.DIM}{' '.join(step['args'][:5])}{Colors.RESET}")

            if step['exit_code'] is not None:
                print(f"  Exit:     {step['exit_code']}")

            # Show output files
            if step['stdout_file']:
                print(f"  Stdout:   {step['stdout_file']}")
            if step['stderr_file']:
                print(f"  Stderr:   {step['stderr_file']}")
            if step['log_file']:
                print(f"  Log:      {step['log_file']}")

            # Show errors/warnings
            if step['errors']:
                print(f"  {Colors.RED}Errors:{Colors.RESET}")
                for err in step['errors']:
                    print(f"    • {err['message']}")

            if step['warnings']:
                print(f"  {Colors.YELLOW}Warnings:{Colors.RESET}")
                for warn in step['warnings']:
                    print(f"    • {warn['message']}")

    def print_files_accessed(self):
        """Print files accessed during execution."""
        files = self.report["files_accessed"]

        if not any(files.values()):
            return

        print(f"\n{Colors.BOLD}Files Accessed:{Colors.RESET}")
        print(f"{Colors.CYAN}{'─'*80}{Colors.RESET}")

        if files['read']:
            print(f"\n{Colors.GREEN}Read ({len(files['read'])} files):{Colors.RESET}")
            for f in files['read'][:10]:  # Show first 10
                print(f"  • {f}")
            if len(files['read']) > 10:
                print(f"  {Colors.DIM}... and {len(files['read']) - 10} more{Colors.RESET}")

        if files['written']:
            print(f"\n{Colors.BLUE}Written ({len(files['written'])} files):{Colors.RESET}")
            for f in files['written'][:10]:
                print(f"  • {f}")
            if len(files['written']) > 10:
                print(f"  {Colors.DIM}... and {len(files['written']) - 10} more{Colors.RESET}")

    def print_performance(self):
        """Print performance metrics."""
        perf = self.report.get("performance", {})

        if not perf:
            return

        print(f"\n{Colors.BOLD}Performance Metrics:{Colors.RESET}")
        print(f"{Colors.CYAN}{'─'*80}{Colors.RESET}")

        if 'cpu_percent' in perf:
            print(f"  CPU Usage:    {perf['cpu_percent']:.1f}%")
        if 'memory_mb' in perf:
            print(f"  Memory Peak:  {perf['memory_mb']:.1f} MB")
        if 'io_read_mb' in perf:
            print(f"  Disk Read:    {perf['io_read_mb']:.1f} MB")
        if 'io_write_mb' in perf:
            print(f"  Disk Write:   {perf['io_write_mb']:.1f} MB")
        if 'num_threads' in perf:
            print(f"  Threads:      {perf['num_threads']}")

    def print_errors_warnings(self):
        """Print all errors and warnings."""
        result = self.report["result"]

        if result['errors']:
            print(f"\n{Colors.RED}{Colors.BOLD}ERRORS ({len(result['errors'])}){Colors.RESET}")
            print(f"{Colors.RED}{'─'*80}{Colors.RESET}")
            for i, err in enumerate(result['errors'], 1):
                print(f"\n{i}. [{err['timestamp']}]")
                print(f"   {err['message']}")

        if result['warnings']:
            print(f"\n{Colors.YELLOW}{Colors.BOLD}WARNINGS ({len(result['warnings'])}){Colors.RESET}")
            print(f"{Colors.YELLOW}{'─'*80}{Colors.RESET}")
            for i, warn in enumerate(result['warnings'], 1):
                print(f"\n{i}. [{warn['timestamp']}]")
                print(f"   {warn['message']}")

    def print_summary(self):
        """Print summary statistics."""
        steps = self.report["execution_flow"]
        result = self.report["result"]

        print(f"\n{Colors.BOLD}Summary Statistics:{Colors.RESET}")
        print(f"{Colors.CYAN}{'─'*80}{Colors.RESET}")

        print(f"  Total Steps:     {len(steps)}")
        print(f"  Total Duration:  {result['duration_total_ms']/1000:.2f}s")

        # Step type breakdown
        step_types = {}
        for step in steps:
            step_types[step['type']] = step_types.get(step['type'], 0) + 1

        print(f"\n  Steps by Type:")
        for stype, count in step_types.items():
            print(f"    {stype:20s} {count}")

        # Slowest steps
        sorted_steps = sorted(steps, key=lambda s: s['duration_ms'], reverse=True)
        print(f"\n  Top 3 Slowest Steps:")
        for i, step in enumerate(sorted_steps[:3], 1):
            print(f"    {i}. {step['name']:30s} {step['duration_ms']/1000:.2f}s")

        # Error summary
        failed_steps = sum(1 for s in steps if s['exit_code'] and s['exit_code'] != 0)
        if failed_steps:
            print(f"\n  {Colors.RED}Failed Steps: {failed_steps}{Colors.RESET}")

    def view(self, errors_only: bool = False, summary_only: bool = False):
        """Display the full report."""
        self.print_header()

        if summary_only:
            self.print_summary()
        else:
            self.print_execution_flow(errors_only)
            self.print_files_accessed()
            self.print_performance()
            self.print_errors_warnings()
            self.print_summary()

        print(f"\n{Colors.CYAN}{'='*80}{Colors.RESET}\n")


def compare_reports(report1_path: Path, report2_path: Path):
    """Compare two debug reports."""
    with open(report1_path) as f:
        r1 = json.load(f)
    with open(report2_path) as f:
        r2 = json.load(f)

    print(f"\n{Colors.CYAN}{'='*80}{Colors.RESET}")
    print(f"{Colors.BOLD}COMPARING REPORTS{Colors.RESET}")
    print(f"{Colors.CYAN}{'='*80}{Colors.RESET}\n")

    print(f"Report 1: {r1['metadata']['test_name']} - {r1['metadata']['timestamp']}")
    print(f"Report 2: {r2['metadata']['test_name']} - {r2['metadata']['timestamp']}\n")

    # Compare durations
    dur1 = r1['result']['duration_total_ms'] / 1000
    dur2 = r2['result']['duration_total_ms'] / 1000
    diff = dur2 - dur1
    pct = (diff / dur1 * 100) if dur1 > 0 else 0

    color = Colors.GREEN if diff < 0 else Colors.RED

    print(f"{Colors.BOLD}Duration Comparison:{Colors.RESET}")
    print(f"  Report 1: {dur1:.2f}s")
    print(f"  Report 2: {dur2:.2f}s")
    print(f"  Diff:     {color}{diff:+.2f}s ({pct:+.1f}%){Colors.RESET}\n")

    # Compare step counts
    print(f"{Colors.BOLD}Steps Comparison:{Colors.RESET}")
    print(f"  Report 1: {len(r1['execution_flow'])} steps")
    print(f"  Report 2: {len(r2['execution_flow'])} steps\n")

    # Compare individual steps
    print(f"{Colors.BOLD}Per-Step Comparison:{Colors.RESET}")
    print(f"{Colors.CYAN}{'─'*80}{Colors.RESET}")

    for i, (s1, s2) in enumerate(zip(r1['execution_flow'], r2['execution_flow']), 1):
        if s1['name'] == s2['name']:
            dur1 = s1['duration_ms'] / 1000
            dur2 = s2['duration_ms'] / 1000
            diff = dur2 - dur1
            pct = (diff / dur1 * 100) if dur1 > 0 else 0

            color = Colors.GREEN if diff < 0 else Colors.RED

            print(f"{i}. {s1['name']:30s} {dur1:6.2f}s → {dur2:6.2f}s "
                  f"{color}({pct:+6.1f}%){Colors.RESET}")

    print(f"{Colors.CYAN}{'='*80}{Colors.RESET}\n")


def find_latest_report(test_name: str = None) -> Path:
    """Find the latest debug report."""
    debug_dir = Path("build/debug_reports")

    if not debug_dir.exists():
        print(f"{Colors.RED}Error: No debug reports found in {debug_dir}{Colors.RESET}")
        sys.exit(1)

    if test_name:
        latest = debug_dir / f"latest_{test_name}.json"
        if latest.exists():
            return latest

    # Find most recent report
    reports = list(debug_dir.glob("run_*.json"))
    if not reports:
        print(f"{Colors.RED}Error: No debug reports found{Colors.RESET}")
        sys.exit(1)

    return max(reports, key=lambda p: p.stat().st_mtime)


def main():
    parser = argparse.ArgumentParser(
        description="Debug Report Viewer for CERES RISC-V",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        "report",
        nargs="?",
        help="Path to debug report JSON (default: latest)"
    )
    parser.add_argument(
        "--test",
        help="View latest report for specific test"
    )
    parser.add_argument(
        "--errors-only",
        action="store_true",
        help="Show only steps with errors"
    )
    parser.add_argument(
        "--summary",
        action="store_true",
        help="Show summary statistics only"
    )
    parser.add_argument(
        "--compare",
        nargs=2,
        metavar=("REPORT1", "REPORT2"),
        help="Compare two reports"
    )

    args = parser.parse_args()

    # Compare mode
    if args.compare:
        compare_reports(Path(args.compare[0]), Path(args.compare[1]))
        return

    # Find report to view
    if args.report:
        report_path = Path(args.report)
    else:
        report_path = find_latest_report(args.test)

    if not report_path.exists():
        print(f"{Colors.RED}Error: Report not found: {report_path}{Colors.RESET}")
        sys.exit(1)

    # View report
    viewer = DebugReportViewer(report_path)
    viewer.view(errors_only=args.errors_only, summary_only=args.summary)


if __name__ == "__main__":
    main()
